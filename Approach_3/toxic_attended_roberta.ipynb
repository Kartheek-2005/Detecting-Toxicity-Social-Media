{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77bdf62-ba28-4af9-89a9-a659706fdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import transformers\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaAttention,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaLayer,\n",
    "    RobertaIntermediate,\n",
    "    RobertaOutput,\n",
    ")\n",
    "from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
    "from typing import List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82a2322-3465-4de4-87ae-059274c6d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicRobertaLayer(nn.Module):\n",
    "    def __init__(self, off_dictionary, config):\n",
    "        super().__init__()\n",
    "        self.device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else: \"cpu\"\n",
    "        self.off_dict = off_dictionary  ###\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = RobertaAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\")\n",
    "            self.toxic_crossattention = RobertaAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.toxic_attention = RobertaAttention(config)  ###\n",
    "        self.intermediate = RobertaIntermediate(config)\n",
    "        self.output = RobertaOutput(config)\n",
    "\n",
    "    def forward_attention(\n",
    "        self,\n",
    "        attention: RobertaAttention,\n",
    "        crossattention: RobertaAttention,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n",
    "                    \" by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_batch: List[str],\n",
    "        tokenizer: RobertaTokenizer,\n",
    "        embeddings: RobertaEmbeddings,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        if self.add_cross_attention: \n",
    "            crossattention = self.crossattention \n",
    "            toxic_crossattention = self.toxic_crossattention\n",
    "        else: \n",
    "            crossattention = None\n",
    "            toxic_crossattention = None\n",
    "            \n",
    "        attn_block1_outputs = self.forward_attention(\n",
    "            self.attention,\n",
    "            crossattention,\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        input_profanities = [\n",
    "            \" \".join([word for word in text.split() if word in self.off_dict])\n",
    "            for text in input_batch\n",
    "        ]\n",
    "        input_prof_tokens = tokenizer(input_profanities, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        input_token_embeddings = embeddings(input_prof_tokens['input_ids'])\n",
    "        attn_block2_outputs = self.forward_attention(\n",
    "            self.toxic_attention,\n",
    "            toxic_crossattention,\n",
    "            input_token_embeddings, \n",
    "            input_prof_tokens['attention_mask'],\n",
    "            head_mask,\n",
    "            encoder_hidden_states, \n",
    "            encoder_attention_mask, \n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        ) \n",
    "        \n",
    "        outputs = (attn_block1_outputs[0] + attn_block2_outputs[0].mean(axis=1, keepdims=True)) / 2\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed25145-dfb4-4b48-8a7d-dac791bfae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicRobertaEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_batch: List[str],\n",
    "        tokenizer: RobertaTokenizer,\n",
    "        embeddings: RobertaEmbeddings,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    input_batch,\n",
    "                    tokenizer,\n",
    "                    embeddings,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    input_batch,\n",
    "                    tokenizer,\n",
    "                    embeddings,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "                \n",
    "            hidden_states = layer_outputs\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03513c8c-be83-4857-9af2-eb7afb59660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaToxicAttentionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        checkpoint: str,\n",
    "        offensive_dict_dir: str\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        loaded_ckpt = torch.hub.load_state_dict_from_url(checkpoint, map_location=self.device)\n",
    "        model_class = getattr(transformers, loaded_ckpt['config']['arch']['args']['model_name'])\n",
    "        config = model_class.config_class.from_pretrained(\n",
    "            loaded_ckpt['config']['arch']['args']['model_type'], \n",
    "            num_labels = loaded_ckpt['config']['arch']['args']['num_classes']\n",
    "        )\n",
    "\n",
    "        self.model = model_class.from_pretrained(\n",
    "            pretrained_model_name_or_path = None, \n",
    "            config = config, \n",
    "            state_dict = loaded_ckpt['state_dict']\n",
    "        )\n",
    "        self.tokenizer = getattr(transformers, loaded_ckpt['config']['arch']['args']['tokenizer_name']).from_pretrained(\n",
    "            pretrained_model_name_or_path = loaded_ckpt['config']['arch']['args']['model_type']\n",
    "        )\n",
    "        new_encoder = ToxicRobertaEncoder(config)\n",
    "        for i, enc_block in enumerate(self.model.roberta.encoder.layer):\n",
    "            toxic_sub_roberta_layer = ToxicRobertaLayer('off_words_cmu.txt', config)\n",
    "            toxic_sub_roberta_layer.attention = enc_block.attention\n",
    "            if config.add_cross_attention: toxic_sub_roberta_layer.crossattention = enc_block.crossattention\n",
    "            new_encoder.layer[i] = toxic_sub_roberta_layer\n",
    "        self.model.roberta.encoder = new_encoder\n",
    "        self.downstream_classifier = nn.Linear(16, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_inputs,\n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True\n",
    "    ):\n",
    "        inputs = self.tokenizer(batch_inputs, return_tensors=return_tensors, truncation=truncation, padding=padding).to(self.model.device)\n",
    "        x = self.model.roberta.embeddings(inputs['input_ids'])\n",
    "        x = self.model.roberta.encoder(batch_inputs, self.tokenizer, self.model.roberta.embeddings, x)\n",
    "        x = self.model.classifier(x[0])\n",
    "        x = self.downstream_classifier(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def freeze_roberta(self):\n",
    "        for param in self.model.roberta.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        for l in self.model.roberta.encoder.layer:\n",
    "            for param in l.attention.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in l.intermediate.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in l.output.parameters():\n",
    "                param.requires_grad = False\n",
    "        for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def backward_pass(self, data, target):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = self.loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def fit(self, epochs, learning_rate, x_train, y_train, x_test: Optional = None, y_test: Optional = None):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.BCELoss()  # Assuming binary classification\n",
    "        \n",
    "        self.freeze_roberta()\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        test_str = \"\"\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch ({epoch + 1} / {epochs}) : {\"=\" * (epoch*100/epochs) + '>'}\", end='\\r')\n",
    "            train_epoch_loss = 0.0\n",
    "            test_epoch_loss = 0.0\n",
    "            for i, (data, target) in enumerate(zip(x_train, y_train)):\n",
    "                data, target = data.to(device), target.to(self.device)\n",
    "                loss = backward_pass(data, target)\n",
    "                train_epoch_loss += loss\n",
    "                if y_test: test_epoch_loss += self.loss(x_test, y_test)\n",
    "            avg_train_epoch_loss = train_epoch_loss / len(y_train)\n",
    "            train_loss.append(avg_train_epoch_loss)\n",
    "            if y_test: \n",
    "                avg_test_epoch_loss = test_epoch_loss / len(y_test)\n",
    "                test_loss.append(avg_test_epoch_loss)\n",
    "                test_str = f\", test_loss = {avg_test_epoch_loss}\"\n",
    "            print(f\"Epoch ({epoch + 1} / {epochs}) : {\"=\" * (epoch*100/epochs) + '>'} | train_loss = {avg_train_epoch_loss}\" + test_str)\n",
    "        loss = {\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss\n",
    "        }\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dc02f1-1cd9-4704-a321-7a1d6e9da3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unitary/unbiased-toxic-roberta\n",
    "checkpoint = \"https://github.com/unitaryai/detoxify/releases/download/v0.3-alpha/toxic_debiased-c7548aa0.ckpt\"\n",
    "device = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "loaded_ckpt = torch.hub.load_state_dict_from_url(checkpoint, map_location=device)\n",
    "model_class = getattr(transformers, loaded_ckpt['config']['arch']['args']['model_name'])\n",
    "config = model_class.config_class.from_pretrained(\n",
    "    loaded_ckpt['config']['arch']['args']['model_type'], \n",
    "    num_labels = loaded_ckpt['config']['arch']['args']['num_classes']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0dc6952-325a-4bcc-8b98-88f1eb943ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## original model\n",
    "# orig_model = model_class.from_pretrained(\n",
    "#     pretrained_model_name_or_path=None,\n",
    "#     config=config,\n",
    "#     state_dict=loaded_ckpt['state_dict'],\n",
    "# )\n",
    "# orig_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6398bc-3754-4ea7-bc28-1b21951dd64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skart\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaToxicAttentionModel(\n",
       "  (model): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): ToxicRobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ToxicRobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (toxic_attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (downstream_classifier): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaToxicAttentionModel(checkpoint, 'off_words_cmu.txt')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef9a2b2-2f05-45e5-9e71-80343afe12d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2538],\n",
       "        [0.2344]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch = ['Mission Impossible', 'Mission Imposter']\n",
    "model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849af80-a8ac-4af8-b53f-d3f0065fd67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
