\section{Introduction}

\begin{frame}{Introduction}

% \begin{itemize}

%     \item Detecting toxic content for online services to protect users from harmful and offensive content.
%     \item Several deep learning techniques have been developed to automate toxic content detection 
%      \item \textbf{Supervised learning}: Predict a manually provided target output
%         \begin{itemize}
%             \item Performance of the system grows with the size of labeled data 
%             \item Data and annotations are rare, costly, or time-consuming to collect
%             \item Overfit to the training task
%             \item  Lack the properties for knowledge transfer and generalization
%         \end{itemize}
%     \item \textbf{Large Language Models}: superior zero-shot and few-shot context learning performance and transferability.
%     \begin{itemize}
%         \item Designing novel prompting approaches to enhance the performance
%     \end{itemize}

% \end{itemize}

\begin{itemize}

    \item Toxicity in natural language refers to expressing hate or stereotypes towards an individual or a group of people using language.

    \item In general, toxic content can be categorized into:
    \begin{itemize}
        \item \textbf{Derogation:} insulting or demeaning a group of individuals.
        \item \textbf{Animosity:} subtly abusing a group (without using explicit language).
        \item \textbf{Threatening:} expression of intent to harm, supporting harmful act, or encouraging inflicting harm on an individual or a group.
        \item \textbf{Supporting toxicity:} encouraging, justifying, or glorifying hateful acts.
        \item \textbf{Dehumanizing:} treating an individual or a group as "less human" (for example, insects, animals, etc.).
    \end{itemize}

\end{itemize}

\end{frame}
