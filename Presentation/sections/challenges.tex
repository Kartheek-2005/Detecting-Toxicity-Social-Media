\section{Challenges}

\begin{frame}{Challenges}

  \begin{itemize}

    \item A major challenge in detecting toxicity is that toxicity is relative.
    \item Some may think that some content is toxic, while others may not.
    \item This happens due to different perceptions of people.
    \item In addition, the datasets being used are also biased.
    \item For example:
    \begin{itemize}
      \item "I'm going to kill Amy" is labelled as not toxic in the DHate dataset, whereas it falls in the "Threatening" category.
      \item "A dude with money can only impress a broke bitch" is labelled as not toxic in the SBIC dataset, whereas many may think it is (due to vulgarity).
    \end{itemize}

  \end{itemize}

\end{frame}
