\section{Challenges}

A major challenge in detecting toxic content is that toxicity is relative.
This means whether something is toxic depends on the person consuming that content.
This happens because different people have different perceptions of toxicity.

In addition, the datasets presented are also biased.
For example:

\begin{itemize}
    \item "I'm going to kill Amy" is labelled as not toxic in the DHate dataset, whereas it falls in the "Threatening" category.
    \item "A dude with money can only impress a broke bitch" is labelled as not toxic in the SBIC dataset, whereas many may think otherwise (due to its vulgarity).
\end{itemize}
