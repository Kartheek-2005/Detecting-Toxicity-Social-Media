\section{Conclusion}

This preliminary report addresses the challenge of detecting toxic content on social media using machine learning techniques. We have explored nearest-neighbour-based methods and the use of large language models to improve classification accuracy. There are Challenges including dataset biases and the subjective nature of toxicity. 
Our efforts will focus on refining models and mitigating biases to enhance robustness across diverse contexts.

\textbf{Toxicity-Attended Roberta Classification} is more model-oriented in detecting toxicity on social media platforms. By leveraging an additional attention block tailored to offensive language, the model achieves heightened sensitivity to nuanced contexts, improving its ability to classify content accurately. The architecture is adaptable and can be fine-tuned for specific domains, ensuring its relevance across various applications.
This approach demonstrates that targeted attention mechanisms can substantially enhance natural language processing tasks, paving the way for more intelligent and context-aware systems.


\section{Supplementary Materials}
Code - https://github.com/NamanChhibbar/Detecting-Toxicity-Social-Media.git
